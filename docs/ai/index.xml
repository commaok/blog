<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Posts - Don&#39;t Panic</title>
    <link>https://commaok.xyz/ai/</link>
    <description>AI-related posts from Don&#39;t Panic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Sep 2025 07:12:48 -0700</lastBuildDate>
    
        <atom:link href="https://commaok.xyz/ai/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Filter outputs and prime the pump</title>
      <link>https://commaok.xyz/ai/context-management/</link>
      <pubDate>Tue, 23 Sep 2025 07:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/context-management/</guid>
      <description>&lt;p&gt;Two small context management tricks.&lt;/p&gt;
&lt;h3 id=&#34;filter-outputs&#34;&gt;Filter outputs&lt;/h3&gt;
&lt;p&gt;Some build tools are noisy. (Yes, I&amp;rsquo;m looking at you, &lt;code&gt;xcodebuild&lt;/code&gt;.)&lt;/p&gt;
&lt;p&gt;We &lt;em&gt;want&lt;/em&gt; our agents to re-build our projects regularly&amp;hellip;but not at the cost of destroying their context window.&lt;/p&gt;
&lt;p&gt;So I had an agent write a tiny shell script wrapper that strips away &lt;em&gt;everything but the error messages&lt;/em&gt; when builds fail. Successful build output is merely: &amp;ldquo;Build succeeded&amp;rdquo;. It uses some awful regexps that only an LLM would write&amp;hellip;but it&amp;rsquo;s fast and it works.&lt;/p&gt;
&lt;p&gt;My token use instantly dropped and the agent got more noticeably effective. And it turns out I prefer it too.&lt;/p&gt;
&lt;p&gt;This is not new! The Unix philosophy says that when a program has nothing to report, it should remain silent.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.catb.org/esr/writings/taoup/html/ch01s06.html#id2878450&#34;&gt;ESR&amp;rsquo;s comment&lt;/a&gt; on this rule is spot-on for both humans and LLMs:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Well-designed programs treat the user&amp;rsquo;s attention and concentration as a precious and limited resource, only to be claimed when necessary.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If your agent runs a build tool that wasn&amp;rsquo;t designed by Rob Pike, invest in a dedicated wrapper to trim its output.&lt;/p&gt;
&lt;h3 id=&#34;prime-the-pump&#34;&gt;Prime the pump&lt;/h3&gt;
&lt;p&gt;Even frontier models don&amp;rsquo;t always listen, particularly if there are several important rules, and some of those rules fly in the face of their instincts (no mocks! use elemwise comparisons, not summary statistics!).&lt;/p&gt;
&lt;p&gt;One way to drive a set of rules home is to ask the agent to start by looking for &lt;em&gt;violations of those rules&lt;/em&gt;, knowing it won&amp;rsquo;t find any. That fills the context window with a bunch of text about those rules, in effect showing&amp;ndash;instead of telling&amp;ndash;the agent what (not) to do.&lt;/p&gt;
&lt;p&gt;This burns useful context window, but it significantly improves compliance. Sometimes this is a worthwhile trade-off. In constrast, I have found the opposite approach (wait for an agent to violate a rule, then ask it to self-correct) to be completely ineffective.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exponentials are not destiny</title>
      <link>https://commaok.xyz/ai/exponential-destiny/</link>
      <pubDate>Thu, 05 Jun 2025 07:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/exponential-destiny/</guid>
      <description>&lt;p&gt;AI enthusiasts (and doomers) like to focus on exponentials.&lt;/p&gt;
&lt;p&gt;Exponentials are worth paying attention to. And humans don&amp;rsquo;t have good intuitions about them.&lt;/p&gt;
&lt;p&gt;But the raw fact of exponential growth means nothing.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Herbert_Stein#Stein&#39;s_Law&#34;&gt;Stein&amp;rsquo;s Law&lt;/a&gt; reminds us:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If something cannot go on forever, it will stop.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Exponentials cannot continue forever. In fact, precisely because of their explosive growth, they&amp;rsquo;re likely to hit their limits quickly.&lt;/p&gt;
&lt;p&gt;At some point, exponential growth meets some form of resource exhaustion, such as food or space for bacteria or fissile material for nuclear reactions. Every exponential is only an exponential until it&amp;rsquo;s a logistic. This is &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_function#Applications&#34;&gt;very well understood&lt;/a&gt;, and shows up repeatedly across a wide variety of fields.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thoughtco.com/thmb/0Uv9YnVuve2zqB0yWr5Cd-CbOIo%3D/1500x1000/filters%3Ano_upscale%28%29%3Amax_bytes%28150000%29%3Astrip_icc%28%29/bacterial_growth_curve-5b56356d4cedfd00371b477b.jpg&#34; alt=&#34;Bacterial growth curve showing exponential and stationary phases&#34;&gt;&lt;/p&gt;
&lt;p&gt;The critical questions when faced with exponential growth are: When, why, and how will it stop? A raw appeal to &amp;ldquo;but exponential!&amp;rdquo; is sloppy at best.&lt;/p&gt;
&lt;p&gt;What resource will AI exhaust? Hard to say. The limitation could be technical, physical, informational, economic, social, political, or something else. When? We don&amp;rsquo;t know. What will the inevitable slowdown look like? Depends. How rapidly will it occur? Shrug. &amp;ldquo;Exponentials!&amp;rdquo; doesn&amp;rsquo;t tell us.&lt;/p&gt;
&lt;p&gt;There, however, is a more nuanced point about exponentials that is worth taking seriously.&lt;/p&gt;
&lt;p&gt;It is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When exponentials are involved, things change extremely rapidly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There&amp;rsquo;s a lag between advances in AI and its visibility to the broader world.&lt;/p&gt;
&lt;p&gt;Diffusion of information takes time: It works its way from people who work at the cutting edge, to people active in the industry, to people interested in the industry, to people aware of the industry, to people for whom AI is nothing but an obnoxiously persistent chyron.&lt;/p&gt;
&lt;p&gt;This lag creates skew between the world we live in, and the world that is already foreseeable.&lt;/p&gt;
&lt;p&gt;And actually adopting and integrating is far slower yet. It takes time for systems to adjust. Rapid changes break things.&lt;/p&gt;
&lt;p&gt;And exponential growth means the rate of change increases in line with the change itself.&lt;/p&gt;
&lt;p&gt;We live in a world where most of us don&amp;rsquo;t know large parts &lt;em&gt;what has happened already&lt;/em&gt;, and &lt;em&gt;what has happened already&lt;/em&gt; will be incredibly disruptive.&lt;/p&gt;
&lt;p&gt;If AI were to freeze, right now, we would continue to feel the shock waves for years to come. The further along the exponential we go, the bigger and longer and less predictable the shock wave will be.&lt;/p&gt;
&lt;p&gt;Exponentials are not destiny; they do not last forever; curves do not predict the future. But they do create informational gaps and high impact changes. That is the fact that warrants attention.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AX: Agent Experience</title>
      <link>https://commaok.xyz/ai/ax/</link>
      <pubDate>Fri, 30 May 2025 07:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/ax/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/User_experience&#34;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;User experience (UX) is how a user interacts with and experiences a product, system, or service.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Developer experience (DX) is a user experience from a developer&amp;rsquo;s point of view.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is time to add:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Agent experience (AX) is a user experience from an AI agent’s point of view.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://sketch.dev/blog/agent-loop&#34;&gt;Agent loops are simple&lt;/a&gt;. But that simplicity hides some depth. An agent&amp;rsquo;s behavior is deeply influenced by its environment. (The same is true of humans.) To build a setting in which an agent is effective, it helps to view the world from its point of view.&lt;/p&gt;
&lt;p&gt;The primary components of AX are obvious and familiar: the system prompt, and the tool selection, naming, design, and documentation. All of these must be designed for how LLMs process information, with &lt;a href=&#34;https://commaok.xyz/ai/prompt-engineering-and-the-taste-gap/&#34;&gt;an strong emphasis on clarity and precision&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are less obvious components, though, such as &lt;a href=&#34;https://sketch.dev/blog/push-pull-respond-restart&#34;&gt;communication design&lt;/a&gt;: what information you give an agent and when.&lt;/p&gt;
&lt;p&gt;And some of it, as with any form of UX, comes down to polish. We &lt;a href=&#34;https://github.com/boldsoftware/sketch/commit/495c1fa247565e21b36bcb847c6cd3f08e0e196f&#34;&gt;just added auto-installation of common tools&lt;/a&gt; to &lt;a href=&#34;https://sketch.dev&#34;&gt;sketch&lt;/a&gt;. If an agent executes yamllint, and yamllint isn’t present, it is silently, magically installed in the container, and the bash command succeeds the first time. Instead of having to pause its work to futz with apt/snap/yum/brew, which is &lt;a href=&#34;https://arxiv.org/abs/2505.06120&#34;&gt;a distraction&lt;/a&gt;, things Just Work. AI agents suffer from yak shaving too.&lt;/p&gt;
&lt;p&gt;AX is in some sense easier than UX/DX, because you have captive users that you can watch any time you want! It is so much easier that we sometimes call it “eval” instead.&lt;/p&gt;
&lt;p&gt;But it’s not just eval. To improve the AX, you have to actually watch and empathize, just like you would with human users…while bearing in mind the ever-changing ways in which they can be decidedly non-human.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Also published at &lt;a href=&#34;https://sketch.dev/blog/ax&#34;&gt;sketch.dev/blog/ax&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prompt Engineering and the Taste Gap</title>
      <link>https://commaok.xyz/ai/prompt-engineering-and-the-taste-gap/</link>
      <pubDate>Thu, 29 May 2025 07:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/prompt-engineering-and-the-taste-gap/</guid>
      <description>&lt;p&gt;As I was learning prompt engineering, I encountered the same (good!) core advice repeatedly: invest in evals, make incremental improvements.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s great. But…where do the words come from? The ones that we evaluate, and iterate on, and augment with examples? When writing code, if you do a good job up front, you&amp;rsquo;ll spend less time fixing bugs and chasing performance issues. How do you do a better job up front of writing and editing a prompt?&lt;/p&gt;
&lt;p&gt;The answer has slowly become clear to me: Have an LLM write and edit your prompts. You are still doing the heavy lifting. The understanding and context you provide is the crux of the work. But an LLM is an unparalleled linguistic optimizer and amanuensis. They are remarkably good at taking muddled, sprawling intentions and crystallizing them into clear, effective prompts.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m a native English speaker with extensive writing experience, most of it technical. I am a reasonably clear thinker and communicator. I&amp;rsquo;ve been writing prompts for a year. And yet frontier models systematically outperform my attempts to do it all myself. Maybe if I learned by osmosis at Anthropic or had another few years of prompt engineering experience this wouldn&amp;rsquo;t be true. But, to a first approximation, that describes nobody, so I&amp;rsquo;m pretty comfortable giving out this advice.&lt;/p&gt;
&lt;p&gt;Frontier LLMs seem to be particularly good at providing strong guardrails to prevent unwanted but stubbornly persistent behaviors. For example, Claude generated this line, which proved to be strikingly effective in its context, and which I would never have come up with: &amp;ldquo;If you are about to do X - stop - and reconsider.&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;how-to-prompt-for-prompts&#34;&gt;How to prompt for prompts&lt;/h2&gt;
&lt;p&gt;When asking an LLM to write a prompt, I bring three things to the table.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Clear instructions.&lt;/strong&gt; I tell it I want it to help me write a prompt. Obvious, but I still sometimes manage to forget this. It is often also fruitful to request that it ask you probing, critical questions. Make it crystal clear you want engagement, not affirmation. (My Claude personal preference begins: &amp;ldquo;I value intellectual, rigorous, critical discussion. I want a tennis partner who returns serves with spin - challenge my assumptions, ask probing questions, suggest better approaches, and engage rather than affirm.&amp;rdquo; This makes Claude charmingly fiesty.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Context.&lt;/strong&gt; Lots and lots of context. What I am trying to accomplish, for whom, and why? I pour in my ideas, dreams, hopes, confusions, plans, fears, draft sentences that seem promising, whatever. I ramble and circumlocute. Get it all down. All of it. Order doesn&amp;rsquo;t matter. It is easy to cut later.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Judgment.&lt;/strong&gt; Ira Glass popularized the Taste Gap, the observation that judgment precedes ability: &amp;ldquo;All of us who do creative work, we get into it because we have good taste. But there is this gap. For the first couple years you make stuff, it&amp;rsquo;s just not that good.&amp;rdquo; The rough draft generated by the LLM usually has obvious flaws: possible misinterpretations, misplaced emphasis, omitted nuance, wrong feel. This is where you can leverage the Taste Gap: It is much easier to identify what&amp;rsquo;s wrong than it is to make it right. Identify everything that doesn&amp;rsquo;t seem right–even if it is hard to nail down precisely–and tell the LLM, and let it iterate. Tweak, quibble, argue, point out shortcomings, cut. Over and over. If the conversation goes off the rails, learn some lessons from it, copy/paste a bunch of text, and &lt;a href=&#34;https://commaok.xyz/ai/push-pull-respond-restart&#34;&gt;start over&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The same approach works for refining a prompt. Explain what you&amp;rsquo;re doing, bring context (the original prompt and what&amp;rsquo;s going wrong), encourage the model not to make major changes (unless you think it&amp;rsquo;s necessary!), put on your judge&amp;rsquo;s wig, and iterate together.&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t just ask an LLM to improve a prompt. If you do this, sure, it&amp;rsquo;ll make a bunch of plausible-sounding changes. But anything it can infer from the original prompt is already implicit in the original prompt. You need to bring new information: new thoughts, observed problems, additional judgments. You could ask it to speculatively expand the prompt and then work to cut it back–the judgment involved in that paring down is new information. But the more context and judgment you bring, the more improvements are available.&lt;/p&gt;
&lt;p&gt;Lastly, don&amp;rsquo;t be afraid to make changes yourself, no matter what the LLM thinks. Trust your judgment. And then, of course, eval!&lt;/p&gt;
&lt;h2 id=&#34;to-each-their-own&#34;&gt;To each their own&lt;/h2&gt;
&lt;p&gt;Oh, one more thing. I have a pet theory: Models have a house palette. Claude responds better to prompts that Claude writes. ChatGPT does a better job following instructions that it wrote.&lt;/p&gt;
&lt;p&gt;I have not put this to an empirical test. (If anyone out there wants to do an experiment&amp;hellip;)&lt;/p&gt;
&lt;p&gt;This applies to humans, too. Words meant for humans should be written by humans.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Also published at &lt;a href=&#34;https://sketch.dev/blog/prompt-engineering-and-the-taste-gap&#34;&gt;sketch.dev/blog/prompt-engineering-and-the-taste-gap&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How and when to talk with your AI Agent</title>
      <link>https://commaok.xyz/ai/push-pull-respond-restart/</link>
      <pubDate>Tue, 27 May 2025 07:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/push-pull-respond-restart/</guid>
      <description>&lt;p&gt;There are obvious similarities between task-oriented communication with a human vs with an AI agent&amp;hellip;and a few places where they diverge. This blog post lays out a simple conceptual framework for thinking about these interactions.&lt;/p&gt;
&lt;p&gt;Suppose I ask a colleague to tackle a task for me. How do I communicate context about that task to them? There are three options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Push: Give them information up front&lt;/li&gt;
&lt;li&gt;Pull: Wait for them to ask me questions&lt;/li&gt;
&lt;li&gt;Respond: Provide feedback on work they&amp;rsquo;ve completed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of these approaches has trade-offs.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s hard to know exactly what information would be useful to Push to them. Obviously they need to know what I have in mind, but telling them extraneous information is a waste of everyone&amp;rsquo;s time, and in extreme cases can be downright obnoxious.&lt;/p&gt;
&lt;p&gt;Pulling is less likely to generate unnecessary communication, but it also generates interruptions for me and adds delays for them. It can take a lot of work to even discover what questions to ask. And there&amp;rsquo;s a difficult balance to strike between over- and under-asking.&lt;/p&gt;
&lt;p&gt;Responding can be very efficient: It is naturally batched, and all the resulting communication is necessary. But it also generates wasted work along the way, and can lead to significant frustration if there were large communication failures earlier.&lt;/p&gt;
&lt;p&gt;In practice, we all use a blend of these approaches. The exact combination depends on the people, the task, the existing shared context, the level of trust, and the relative cost of communication, interruptions, and work.&lt;/p&gt;
&lt;h2 id=&#34;using-agents&#34;&gt;Using agents&lt;/h2&gt;
&lt;p&gt;This same set of options appears when asking an AI agent to do work for you.&lt;/p&gt;
&lt;p&gt;The more information you Push to them up front, the more likely you&amp;rsquo;ll get a better outcome, but there are rapidly diminishing returns. Cutting-edge AI agent systems are capable of figuring out quite a lot on their own from surprisingly little context.&lt;/p&gt;
&lt;p&gt;As the AI agents get more autonomous, though, they are used for bigger chunks of work, so they get slower, which means that you don&amp;rsquo;t want to wait around to be available for Pulling, and interruptions are disruptive. And as agents tackle more on their own, it makes sense to launch more of them concurrently, which means that interruptions can easily saturate your focus bandwidth and sap your productivity.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve started doing more at Response time, and worrying less about wasted work. An agent&amp;rsquo;s work is cheap (compared to my time) and getting cheaper, and I can always just spin up more agents.&lt;/p&gt;
&lt;p&gt;With agents, there&amp;rsquo;s also a fourth option:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Restart: Learn from the attempt, throw away everything, and start from scratch, Pushing the extra information that was missing the first time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(You can also do this with humans, but it&amp;rsquo;s&amp;hellip;not very nice.)&lt;/p&gt;
&lt;p&gt;Adding Restart into the mix turns out to be surprisingly effective. Push a tiny bit of information, Respond only with significant feedback, learn from the result, Restart with better initial guidance, repeat.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://maruel.ca&#34;&gt;Marc-Antoine Ruel&lt;/a&gt; &lt;a href=&#34;https://discord.com/channels/1362869091156758752/1362869091156758755/1369809064120553472&#34;&gt;explained this approach well&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Often I ask it to create a change, look at the diff, realize it&amp;rsquo;s wrong but it gave me an idea. I delete the branch and write the correct code manually. Then once it&amp;rsquo;s scaffolded correctly, sometimes I let it finish the job. I find this faster (less draining emotionally?) than trying to argue.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(If you&amp;rsquo;re skeptical about having AI-written code in your repo, this may be a thing to try. Let the AI run ahead, make mistakes, pave a path, and convince you that the game is worth the candle. Then write the code yourself, fore-warned and fore-armed.)&lt;/p&gt;
&lt;h2 id=&#34;designing-agents&#34;&gt;Designing Agents&lt;/h2&gt;
&lt;p&gt;When building an AI agent, you encounter the same set of considerations, but from the design perspective. The product UX choices strongly influence when and how communication occurs.&lt;/p&gt;
&lt;p&gt;For example, OpenAI&amp;rsquo;s &lt;a href=&#34;https://openai.com/index/introducing-deep-research/&#34;&gt;DeepResearch&lt;/a&gt; settled on the user Pushing their question, then one immediate round of Pull, and then presumably Respond. (I personally have never done any Respond iterations with DeepResearch.) They judged that the &lt;a href=&#34;https://twimlai.com/podcast/twimlai/how-openai-builds-ai-agents-that-think-and-act/&#34;&gt;quick Pull improved results&lt;/a&gt; meaningfully without delaying or interrupting too much.&lt;/p&gt;
&lt;p&gt;Here at Sketch (an AI coding agent), for human interactions, we&amp;rsquo;ve focused mainly on Respond and Restart. Yes, Pushed information to specify the task is necessary, but we want to be able to work well even with very little provided up front.&lt;/p&gt;
&lt;p&gt;AI agents interact with their environments, and the same communication design questions arise for &lt;em&gt;agent-environment&lt;/em&gt; communication as for &lt;em&gt;agent-human&lt;/em&gt; communication. Take commit message styles; that information is available from git without bothering a human. We could analyze those and Push it to the agent at the very beginning. That&amp;rsquo;s predictable and reliable, but is irrelevant overhead and distraction if no commits get created. Or we could Respond by asking it to rewrite a commit message if/when we automatically detect that it is not in the desired style. (Restarting is always human-initiated, so that&amp;rsquo;s out.) In practice, we let the agent Pull that information when it is ready to write commits. And we have guard rails: If the agent hasn&amp;rsquo;t Pulled style information, we refuse to let it commit.&lt;/p&gt;
&lt;p&gt;The communication design is a prime question for us when adding any new feature. We employ a combination of all approaches, on a case by case basis, based on what happens to work well.&lt;/p&gt;
&lt;p&gt;The field of Agent Experience (AX) is mostly ad hoc, currently mostly confined to prompt engineering and MCP server design. It will likely grow.&lt;/p&gt;
&lt;p&gt;AI agents have hit the mainstream, but as an industry we have yet to develop a clear theory (that I have seen) around communication design, both UX and AX. Hopefully framing this question will help open up the discussion.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Also published at &lt;a href=&#34;https://sketch.dev/blog/push-pull-respond-restart&#34;&gt;sketch.dev/blog/push-pull-respond-restart&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is Claude a compiler?</title>
      <link>https://commaok.xyz/ai/is-claude-a-compiler/</link>
      <pubDate>Fri, 23 May 2025 15:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/is-claude-a-compiler/</guid>
      <description>&lt;p&gt;I was lucky enough to attend the Code with Claude conference yesterday.&lt;/p&gt;
&lt;p&gt;The hands-down highlight was the talk by &lt;a href=&#34;https://erikschluntz.com&#34;&gt;Erik Schluntz&lt;/a&gt;: Vibe coding in prod.&lt;/p&gt;
&lt;p&gt;Among other things, he drew an analogy between LLMs and compilers.&lt;/p&gt;
&lt;p&gt;Once upon a time, people wrote assembly. No more.&lt;/p&gt;
&lt;p&gt;Compilers are better. Compiler bugs got ironed out, and trust developed. Not many people write assembly by hand any more. (I am one of them, at times.)&lt;/p&gt;
&lt;p&gt;So, he says, it will be with Claude. It&amp;rsquo;ll get more reliable, we&amp;rsquo;ll develop trust, and eventually, not many people will write code by hand. (Will I be one of them? I already mostly vibe-code.)&lt;/p&gt;
&lt;p&gt;There are a few obvious ways in which this analogy breaks down. I want to explore a few of them, as a means of taking his arguments seriously.&lt;/p&gt;
&lt;h1 id=&#34;reliability&#34;&gt;Reliability&lt;/h1&gt;
&lt;p&gt;Compiler reliability is extremely, extremely high. Got a bug? It is in your code, not the compiler. (Until it&amp;rsquo;s not. I&amp;rsquo;ve personally authored enough compiler bugs to last several lifetimes.)&lt;/p&gt;
&lt;p&gt;Will LLM reliability get that high? I&amp;rsquo;m definitely not going to say no. I take exponential curves seriously.&lt;/p&gt;
&lt;p&gt;But one major challenge is that the way that compilers get so reliable is through sheer code miles.&lt;/p&gt;
&lt;p&gt;You can backtest a compiler with a phenomenal amount of code. The implicit regression test is massive.&lt;/p&gt;
&lt;p&gt;LLMs are non-deterministic. The really hard bugs to find and fix in a toolchain are non-deterministic ones due to things like memory corruption and race conditions.&lt;/p&gt;
&lt;p&gt;The rarer and more heisen the bug, the greater the effort required to find and address it. If there are only a few rare bugs, you might decide to live with them. But if there&amp;rsquo;s a fat tail of rare bugs, the ride gets bumpy. (This is true of human health. Any &lt;em&gt;given&lt;/em&gt; rare disease is rare, but having &lt;em&gt;a&lt;/em&gt; rare disease is not uncommon.)&lt;/p&gt;
&lt;p&gt;It is an open, empirical question what distribution LLM reliability problems have. But there could be dragons here.&lt;/p&gt;
&lt;p&gt;Setting aside determinism, LLMs are also orders of magnitude more compute intensive to run. Backtesting at a scale close to compilers is cost prohibitive.&lt;/p&gt;
&lt;p&gt;Will hardware advances save us? With exponential cost reductions and speed improvements, today&amp;rsquo;s cost prohibitive is tomorrow&amp;rsquo;s chump change.&lt;/p&gt;
&lt;p&gt;There are two exponentials at play here, though: exponential hardware improvements and exponential AI quality improvements. And they are at odds with each other. If the exponential AI quality improvements are predicated on scaling laws, then exponential hardware improvement needs to have a bigger exponent than AI quality improvement. Given the limits of physics, there might be dragons lurking here too.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s also a second-order reliability effect. In addition to not introducing bugs, compilers eliminate entire classes of bugs, as do memory-safe languages. These effects compound. LLMs still generate more severe bugs than humans, not just merely more bugs.&lt;/p&gt;
&lt;h1 id=&#34;trust&#34;&gt;Trust&lt;/h1&gt;
&lt;p&gt;I trust the compiler a &lt;em&gt;lot&lt;/em&gt; more than I trust Claude. And I trust Claude more than any other LLM.&lt;/p&gt;
&lt;p&gt;Most trust is, in practice, earned from reliability.&lt;/p&gt;
&lt;p&gt;But there&amp;rsquo;s also supply chain security style trust. That&amp;rsquo;s more challenging.&lt;/p&gt;
&lt;p&gt;Compilers are not immune. &lt;a href=&#34;https://research.swtch.com/nih&#34;&gt;Trusting Trust&lt;/a&gt; is very old and has aged well.&lt;/p&gt;
&lt;p&gt;But compilers do not have quite the scope for mischief that LLMs do.&lt;/p&gt;
&lt;p&gt;We understand compilers well. They are highly interpretable. And the attack surface area is small compared to an internet&amp;rsquo;s worth of training text.&lt;/p&gt;
&lt;p&gt;And determinism helps immensely. Security breaches are bad, but undetected security breaches are far worse. The fact that the &lt;a href=&#34;https://en.wikipedia.org/wiki/XZ_Utils_backdoor&#34;&gt;xz attack&lt;/a&gt; was caught due to a minor performance impact is telling.&lt;/p&gt;
&lt;p&gt;Lastly, the very same exponentials that drive LLM performance cut the other way too, at least right now: The more powerful the model, the higher the risk of invisible security problems.&lt;/p&gt;
&lt;p&gt;I hope we develop a level of confidence and insight into LLMs comparable to compilers. It is early days.&lt;/p&gt;
&lt;p&gt;The reality is, though, that I trust my compiler (in the security sense) mainly because I have no choice. Our lives are built extremely deeply around trust, from the food we eat to the multi-ton chunks of steel that we ignore as they whizz past us, mere feet away. So it may soon be with LLMs.&lt;/p&gt;
&lt;h1 id=&#34;so&#34;&gt;So&amp;hellip;?&lt;/h1&gt;
&lt;p&gt;Where does that leave us?&lt;/p&gt;
&lt;p&gt;I believe, like Erik, that the decline of hand-written code will accelerate, soon, possibly precipitously, but mainly for economic reasons. LLMs are dramatically cheaper to run than humans are to employ.&lt;/p&gt;
&lt;p&gt;We are rapidly moving towards a world in which LLMs write the code, whether the reliability and trust can match compilers or not. The era of bespoke software is upon us.&lt;/p&gt;
&lt;p&gt;We are still paying the price for building the internet and other foundational software technologies&amp;hellip;and then tacking on security afterwards. This pattern looks set to repeat with LLMs. Long tail risks are notoriously hard to find funding for: There are always competitors happy to run risks, and executives happy to throw away their umbrellas in a rainstorm because they&amp;rsquo;re not getting wet.&lt;/p&gt;
&lt;p&gt;If LLMs can&amp;rsquo;t make up the missing ground with compilers, and fast, there will be a lot of whiplash.&lt;/p&gt;
&lt;p&gt;We all have work to do.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blog schism</title>
      <link>https://commaok.xyz/ai/blog-schism/</link>
      <pubDate>Fri, 23 May 2025 05:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/blog-schism/</guid>
      <description>&lt;p&gt;Lots of people don&amp;rsquo;t want to read about AI.&lt;/p&gt;
&lt;p&gt;I respect that.&lt;/p&gt;
&lt;p&gt;But I&amp;rsquo;m currently steeped in the world of AI, for better or for worse, and I want to blog about it. So I&amp;rsquo;ve split this blog in half.&lt;/p&gt;
&lt;p&gt;You are reading the AI half.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s another, at &lt;a href=&#34;https://commaok.xyz/&#34;&gt;/&lt;/a&gt; that contains non-AI posts. It has its own RSS feed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Of manners and machines</title>
      <link>https://commaok.xyz/ai/manners/</link>
      <pubDate>Thu, 03 Apr 2025 10:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/manners/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;lsquo;A person who is nice to you, but rude to the waiter, is not a nice person.&amp;rsquo;
– Dave Barry&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I hate typing. I have longstanding RSI issues. If not carefully managed, the pain can be debilitating. I have occasionally wondered whether I will have to give up a career I love. (Hat tip to &lt;a href=&#34;https://www.cursorless.org&#34;&gt;Cursorless&lt;/a&gt; for rescuing me in the past.)&lt;/p&gt;
&lt;p&gt;And yet I do not save keystrokes by being curt online. There’s a &lt;a href=&#34;https://xkcd.com/438/&#34;&gt;human out there on the other side&lt;/a&gt;, reading. To be fair, I&amp;rsquo;m not an angel. But I try.&lt;/p&gt;
&lt;p&gt;But what if it’s not a human? What if it’s a private, task-oriented, throwaway conversation with an LLM?&lt;/p&gt;
&lt;p&gt;With early LLMs, people discovered that you&amp;rsquo;d get better results if you threatened them with the loss of their job or the death of a kitten. You can also offer to bribe them (with no intent or ability to follow through). These are things only a sociopath would say to another human, particularly for a comically low-stakes task like &amp;ldquo;write me a poem about wombats&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;That era was blessedly short-lived. Modern LLMs are reliable and helpful. They are also incredibly resilient. You&amp;rsquo;ll get a similar quality response to &amp;ldquo;error handling sucks&amp;rdquo; as you will to &amp;ldquo;please make the error handling more robust&amp;rdquo;. It’s just a pile of FLOPS on the other side, so why not cut to the chase, and maybe blow off some steam along the way? (Why &lt;em&gt;does&lt;/em&gt; it take more words to be polite?)&lt;/p&gt;
&lt;p&gt;Nevertheless, I am polite to LLMs. Not for the sake of the machine, but for me. To mangle Dave Barry:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A person who is nice to people, but rude to LLMs, becomes a less nice person.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is not a new idea.&lt;/p&gt;
&lt;p&gt;A significant strand in Aristotelian ethics is that our character is formed through repeated actions, that our habits become who we are. It shows up (Claude tells me) in the idea of karma: &amp;ldquo;a man of good acts will become good, a man of bad acts, bad&amp;rdquo;. And in the Japanese notion of kata: our actions emerge naturally from well-worn patterns. And most parents have extensive first-hand experience of attempting to guide their child&amp;rsquo;s character by responding patiently but firmly to rudeness.&lt;/p&gt;
&lt;p&gt;I would actively prefer to use a model that is not tolerant of blatant rudeness. If I&amp;rsquo;m acting like a jerk, it&amp;rsquo;s valuable, if difficult, for my friends and family to gently push back. On the flip side, if AI assistants act like servants, that may encourage people to treat them accordingly, perpetuating or even deepening the problem. I would even speculate that a model that stands up for itself might fare better on responsible use in general (broken windows theory, watching eye effect). And as AI starts acting in a friendship role for some people, this becomes all the more important.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>