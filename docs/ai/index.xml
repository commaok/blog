<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Posts - Don&#39;t Panic</title>
    <link>https://commaok.xyz/ai/</link>
    <description>AI-related posts from Don&#39;t Panic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Apr 2025 10:12:48 -0700</lastBuildDate>
    
        <atom:link href="https://commaok.xyz/ai/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Of manners and machines</title>
      <link>https://commaok.xyz/ai/manners/</link>
      <pubDate>Thu, 03 Apr 2025 10:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/manners/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;lsquo;A person who is nice to you, but rude to the waiter, is not a nice person.&amp;rsquo;
– Dave Barry&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I hate typing. I have longstanding RSI issues. If not carefully managed, the pain can be debilitating. I have occasionally wondered whether I will have to give up a career I love. (Hat tip to &lt;a href=&#34;https://www.cursorless.org&#34;&gt;Cursorless&lt;/a&gt; for rescuing me in the past.)&lt;/p&gt;
&lt;p&gt;And yet I do not save keystrokes by being curt online. There’s a &lt;a href=&#34;https://xkcd.com/438/&#34;&gt;human out there on the other side&lt;/a&gt;, reading. To be fair, I&amp;rsquo;m not an angel. But I try.&lt;/p&gt;
&lt;p&gt;But what if it’s not a human? What if it’s a private, task-oriented, throwaway conversation with an LLM?&lt;/p&gt;
&lt;p&gt;With early LLMs, people discovered that you&amp;rsquo;d get better results if you threatened them with the loss of their job or the death of a kitten. You can also offer to bribe them (with no intent or ability to follow through). These are things only a sociopath would say to another human, particularly for a comically low-stakes task like &amp;ldquo;write me a poem about wombats&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;That era was blessedly short-lived. Modern LLMs are reliable and helpful. They are also incredibly resilient. You&amp;rsquo;ll get a similar quality response to &amp;ldquo;error handling sucks&amp;rdquo; as you will to &amp;ldquo;please make the error handling more robust&amp;rdquo;. It’s just a pile of FLOPS on the other side, so why not cut to the chase, and maybe blow off some steam along the way? (Why &lt;em&gt;does&lt;/em&gt; it take more words to be polite?)&lt;/p&gt;
&lt;p&gt;Nevertheless, I am polite to LLMs. Not for the sake of the machine, but for me. To mangle Dave Barry:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A person who is nice to people, but rude to LLMs, becomes a less nice person.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is not a new idea.&lt;/p&gt;
&lt;p&gt;A significant strand in Aristotelian ethics is that our character is formed through repeated actions, that our habits become who we are. It shows up (Claude tells me) in the idea of karma: &amp;ldquo;a man of good acts will become good, a man of bad acts, bad&amp;rdquo;. And in the Japanese notion of kata: our actions emerge naturally from well-worn patterns. And most parents have extensive first-hand experience of attempting to guide their child&amp;rsquo;s character by responding patiently but firmly to rudeness.&lt;/p&gt;
&lt;p&gt;I would actively prefer to use a model that is not tolerant of blatant rudeness. If I&amp;rsquo;m acting like a jerk, it&amp;rsquo;s valuable, if difficult, for my friends and family to gently push back. On the flip side, if AI assistants act like servants, that may encourage people to treat them accordingly, perpetuating or even deepening the problem. I would even speculate that a model that stands up for itself might fare better on responsible use in general (broken windows theory, watching eye effect). And as AI starts acting in a friendship role for some people, this becomes all the more important.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What even is varentropy?</title>
      <link>https://commaok.xyz/ai/entropix/</link>
      <pubDate>Thu, 17 Oct 2024 15:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/entropix/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/xjdr-alt/entropix&#34;&gt;Entropix&lt;/a&gt; aims to improves LLM token sampling by incorporating &amp;ldquo;varentropy&amp;rdquo; into its decision making.&lt;/p&gt;
&lt;p&gt;According to the README, as of Oct 18, 2024:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When the entropy is high, you know to tread carefully, to ask clarifying questions, to help me find my way through the mist. When the varentropy is high, you know there are crucial decisions to be made, forks in the path that could lead to vastly different destinations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I have solid intuitions about entropy. But what is varentropy?&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I asked an LLM to write me some &lt;a href=&#34;https://gist.github.com/josharian/c239f74181be3b580e4c5911068d2446&#34;&gt;visualization code&lt;/a&gt; for entropy and varentropy in simplified conditions, using varentropy as implemented in the entropix repo.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a graph of entropy and varentropy for choosing between two outcomes. (LLMorse, anyone?)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://commaok.xyz/images/entropy_vs_varentropy_two_outcomes.png&#34; alt=&#34;Entropy and varentropy for choosing between two outcomes&#34;&gt;&lt;/p&gt;
&lt;p&gt;At 50/50 odds, entropy is at its highest (1 bit). We know nothing going in. At the extremes, entropy is 0, because the outcome is predetermined.&lt;/p&gt;
&lt;p&gt;What does varentropy look like? It is zero at 50/50 and zero at the extremes. It is only high where we&amp;rsquo;re quite confident in the outcome (a bit over 90%), but not certain.&lt;/p&gt;
&lt;p&gt;That doesn&amp;rsquo;t really match up well with the intuitive description of varentropy in the readme. These aren&amp;rsquo;t &amp;ldquo;forks in the path&amp;rdquo;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Things often get way more interesting as you add dimensions. I had the LLM update &lt;a href=&#34;https://gist.github.com/josharian/498a5951462721fa5a214d685dc32a53&#34;&gt;the visualization&lt;/a&gt; to add one more outcome to the graph to see what happens.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://commaok.xyz/images/entropy_vs_varentropy_three_outcomes.png&#34; alt=&#34;Entropy and varentropy for choosing between three outcomes&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is a bit harder to read, but each corner of the triangle corresponds to certainty in a particular outcome. The middle of the triangle is complete uncertainty. The color tells us both the entropy and varentropy. Black is low entropy, low varentropy. Red is high entropy, low varentropy. (The colorspace choices are clearly imperfect, but it&amp;rsquo;s enough to get a feel.)&lt;/p&gt;
&lt;p&gt;This looks more or less the same as the two-outcome graph. Complete uncertainty is high entropy, low varentropy, including when we can mostly rule out one outcome but are uncertain between the other two. Complete certainty is low entropy, low varentropy (but it&amp;rsquo;s a tiny, tiny part of the graph now!). And high varentropy occurs when there&amp;rsquo;s a pretty clear choice, but not certain.&lt;/p&gt;
&lt;p&gt;But the README says this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;High varentropy means I&amp;rsquo;m considering vastly different futures, different tones and directions. Low varentropy means I&amp;rsquo;m more sure of the general shape, even if the specifics are still obscured.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This doesn&amp;rsquo;t track.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Looking at these graphs, and staring at the varentropy calculation, this overall pattern will continue into higher dimensions.&lt;/p&gt;
&lt;p&gt;I see no connection between the imagery and intuition in the README and the actual calculations in practice.&lt;/p&gt;
&lt;p&gt;Maybe the imagery is misleading, but the usage is clear? According to the chart in the README, the sampler chooses between four options:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;* Low entropy, low varentropy: Argmax / greedy
* Low entropy, high varentropy: Branch
* High entropy, low varentropy: Pause / chain-of-thought
* High entropy, high varentropy: Resample
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems plausible that low entropy is a signal to sample greedily, and high entropy indicates you should do something more expensive. I see no particular reason to think that varentropy helps distinguish between which more expensive thing to do, though.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This doesn&amp;rsquo;t mean that the entropix sampler doesn&amp;rsquo;t work. I don&amp;rsquo;t know whether it does.&lt;/p&gt;
&lt;p&gt;But if, empirically, the sampler yields superior results, that means one of three things is true:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;My failure to understand is just that.&lt;/li&gt;
&lt;li&gt;We need a better intuition for varentropy.&lt;/li&gt;
&lt;li&gt;We need a better understanding of why it works, one that might or might not have anything to do with varentropy.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>