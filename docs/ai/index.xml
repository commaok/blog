<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Posts - Don&#39;t Panic</title>
    <link>https://commaok.xyz/ai/</link>
    <description>AI-related posts from Don&#39;t Panic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 May 2025 07:12:48 -0700</lastBuildDate>
    
        <atom:link href="https://commaok.xyz/ai/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How and when to talk with your AI Agent</title>
      <link>https://commaok.xyz/ai/push-pull-respond-restart/</link>
      <pubDate>Tue, 27 May 2025 07:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/push-pull-respond-restart/</guid>
      <description>&lt;p&gt;There are obvious similarities between task-oriented communication with a human vs with an AI agent&amp;hellip;and a few places where they diverge. This blog post lays out a simple conceptual framework for thinking about these interactions.&lt;/p&gt;
&lt;p&gt;Suppose I ask a colleague to tackle a task for me. How do I communicate context about that task to them? There are three options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Push: Give them information up front&lt;/li&gt;
&lt;li&gt;Pull: Wait for them to ask me questions&lt;/li&gt;
&lt;li&gt;Respond: Provide feedback on work they&amp;rsquo;ve completed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of these approaches has trade-offs.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s hard to know exactly what information would be useful to Push to them. Obviously they need to know what I have in mind, but telling them extraneous information is a waste of everyone&amp;rsquo;s time, and in extreme cases can be downright obnoxious.&lt;/p&gt;
&lt;p&gt;Pulling is less likely to generate unnecessary communication, but it also generates interruptions for me and adds delays for them. It can take a lot of work to even discover what questions to ask. And there&amp;rsquo;s a difficult balance to strike between over- and under-asking.&lt;/p&gt;
&lt;p&gt;Responding can be very efficient: It is naturally batched, and all the resulting communication is necessary. But it also generates wasted work along the way, and can lead to significant frustration if there were large communication failures earlier.&lt;/p&gt;
&lt;p&gt;In practice, we all use a blend of these approaches. The exact combination depends on the people, the task, the existing shared context, the level of trust, and the relative cost of communication, interruptions, and work.&lt;/p&gt;
&lt;h2 id=&#34;using-agents&#34;&gt;Using agents&lt;/h2&gt;
&lt;p&gt;This same set of options appears when asking an AI agent to do work for you.&lt;/p&gt;
&lt;p&gt;The more information you Push to them up front, the more likely you&amp;rsquo;ll get a better outcome, but there are rapidly diminishing returns. Cutting-edge AI agent systems are capable of figuring out quite a lot on their own from surprisingly little context.&lt;/p&gt;
&lt;p&gt;As the AI agents get more autonomous, though, they are used for bigger chunks of work, so they get slower, which means that you don&amp;rsquo;t want to wait around to be available for Pulling, and interruptions are disruptive. And as agents tackle more on their own, it makes sense to launch more of them concurrently, which means that interruptions can easily saturate your focus bandwidth and sap your productivity.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve started doing more at Response time, and worrying less about wasted work. An agent&amp;rsquo;s work is cheap (compared to my time) and getting cheaper, and I can always just spin up more agents.&lt;/p&gt;
&lt;p&gt;With agents, there&amp;rsquo;s also a fourth option:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Restart: Learn from the attempt, throw away everything, and start from scratch, Pushing the extra information that was missing the first time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(You can also do this with humans, but it&amp;rsquo;s&amp;hellip;not very nice.)&lt;/p&gt;
&lt;p&gt;Adding Restart into the mix turns out to be surprisingly effective. Push a tiny bit of information, Respond only with significant feedback, learn from the result, Restart with better initial guidance, repeat.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://maruel.ca&#34;&gt;Marc-Antoine Ruel&lt;/a&gt; &lt;a href=&#34;https://discord.com/channels/1362869091156758752/1362869091156758755/1369809064120553472&#34;&gt;explained this approach well&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Often I ask it to create a change, look at the diff, realize it&amp;rsquo;s wrong but it gave me an idea. I delete the branch and write the correct code manually. Then once it&amp;rsquo;s scaffolded correctly, sometimes I let it finish the job. I find this faster (less draining emotionally?) than trying to argue.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(If you&amp;rsquo;re skeptical about having AI-written code in your repo, this may be a thing to try. Let the AI run ahead, make mistakes, pave a path, and convince you that the game is worth the candle. Then write the code yourself, fore-warned and fore-armed.)&lt;/p&gt;
&lt;h2 id=&#34;designing-agents&#34;&gt;Designing Agents&lt;/h2&gt;
&lt;p&gt;When building an AI agent, you encounter the same set of considerations, but from the design perspective. The product UX choices strongly influence when and how communication occurs.&lt;/p&gt;
&lt;p&gt;For example, OpenAI&amp;rsquo;s &lt;a href=&#34;https://openai.com/index/introducing-deep-research/&#34;&gt;DeepResearch&lt;/a&gt; settled on the user Pushing their question, then one immediate round of Pull, and then presumably Respond. (I personally have never done any Respond iterations with DeepResearch.) They judged that the &lt;a href=&#34;https://twimlai.com/podcast/twimlai/how-openai-builds-ai-agents-that-think-and-act/&#34;&gt;quick Pull improved results&lt;/a&gt; meaningfully without delaying or interrupting too much.&lt;/p&gt;
&lt;p&gt;Here at Sketch (an AI coding agent), for human interactions, we&amp;rsquo;ve focused mainly on Respond and Restart. Yes, Pushed information to specify the task is necessary, but we want to be able to work well even with very little provided up front.&lt;/p&gt;
&lt;p&gt;AI agents interact with their environments, and the same communication design questions arise for &lt;em&gt;agent-environment&lt;/em&gt; communication as for &lt;em&gt;agent-human&lt;/em&gt; communication. Take commit message styles; that information is available from git without bothering a human. We could analyze those and Push it to the agent at the very beginning. That&amp;rsquo;s predictable and reliable, but is irrelevant overhead and distraction if no commits get created. Or we could Respond by asking it to rewrite a commit message if/when we automatically detect that it is not in the desired style. (Restarting is always human-initiated, so that&amp;rsquo;s out.) In practice, we let the agent Pull that information when it is ready to write commits. And we have guard rails: If the agent hasn&amp;rsquo;t Pulled style information, we refuse to let it commit.&lt;/p&gt;
&lt;p&gt;The communication design is a prime question for us when adding any new feature. We employ a combination of all approaches, on a case by case basis, based on what happens to work well.&lt;/p&gt;
&lt;p&gt;The field of Agent Experience (AX) is mostly ad hoc, currently mostly confined to prompt engineering and MCP server design. It will likely grow.&lt;/p&gt;
&lt;p&gt;AI agents have hit the mainstream, but as an industry we have yet to develop a clear theory (that I have seen) around communication design, both UX and AX. Hopefully framing this question will help open up the discussion.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Also published at &lt;a href=&#34;https://sketch.dev/blog/push-pull-respond-restart&#34;&gt;sketch.dev/blog/push-pull-respond-restart&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is Claude a compiler?</title>
      <link>https://commaok.xyz/ai/is-claude-a-compiler/</link>
      <pubDate>Fri, 23 May 2025 15:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/is-claude-a-compiler/</guid>
      <description>&lt;p&gt;I was lucky enough to attend the Code with Claude conference yesterday.&lt;/p&gt;
&lt;p&gt;The hands-down highlight was the talk by &lt;a href=&#34;https://erikschluntz.com&#34;&gt;Erik Schluntz&lt;/a&gt;: Vibe coding in prod.&lt;/p&gt;
&lt;p&gt;Among other things, he drew an analogy between LLMs and compilers.&lt;/p&gt;
&lt;p&gt;Once upon a time, people wrote assembly. No more.&lt;/p&gt;
&lt;p&gt;Compilers are better. Compiler bugs got ironed out, and trust developed. Not many people write assembly by hand any more. (I am one of them, at times.)&lt;/p&gt;
&lt;p&gt;So, he says, it will be with Claude. It&amp;rsquo;ll get more reliable, we&amp;rsquo;ll develop trust, and eventually, not many people will write code by hand. (Will I be one of them? I already mostly vibe-code.)&lt;/p&gt;
&lt;p&gt;There are a few obvious ways in which this analogy breaks down. I want to explore a few of them, as a means of taking his arguments seriously.&lt;/p&gt;
&lt;h1 id=&#34;reliability&#34;&gt;Reliability&lt;/h1&gt;
&lt;p&gt;Compiler reliability is extremely, extremely high. Got a bug? It is in your code, not the compiler. (Until it&amp;rsquo;s not. I&amp;rsquo;ve personally authored enough compiler bugs to last several lifetimes.)&lt;/p&gt;
&lt;p&gt;Will LLM reliability get that high? I&amp;rsquo;m definitely not going to say no. I take exponential curves seriously.&lt;/p&gt;
&lt;p&gt;But one major challenge is that the way that compilers get so reliable is through sheer code miles.&lt;/p&gt;
&lt;p&gt;You can backtest a compiler with a phenomenal amount of code. The implicit regression test is massive.&lt;/p&gt;
&lt;p&gt;LLMs are non-deterministic. The really hard bugs to find and fix in a toolchain are non-deterministic ones due to things like memory corruption and race conditions.&lt;/p&gt;
&lt;p&gt;The rarer and more heisen the bug, the greater the effort required to find and address it. If there are only a few rare bugs, you might decide to live with them. But if there&amp;rsquo;s a fat tail of rare bugs, the ride gets bumpy. (This is true of human health. Any &lt;em&gt;given&lt;/em&gt; rare disease is rare, but having &lt;em&gt;a&lt;/em&gt; rare disease is not uncommon.)&lt;/p&gt;
&lt;p&gt;It is an open, empirical question what distribution LLM reliability problems have. But there could be dragons here.&lt;/p&gt;
&lt;p&gt;Setting aside determinism, LLMs are also orders of magnitude more compute intensive to run. Backtesting at a scale close to compilers is cost prohibitive.&lt;/p&gt;
&lt;p&gt;Will hardware advances save us? With exponential cost reductions and speed improvements, today&amp;rsquo;s cost prohibitive is tomorrow&amp;rsquo;s chump change.&lt;/p&gt;
&lt;p&gt;There are two exponentials at play here, though: exponential hardware improvements and exponential AI quality improvements. And they are at odds with each other. If the exponential AI quality improvements are predicated on scaling laws, then exponential hardware improvement needs to have a bigger exponent than AI quality improvement. Given the limits of physics, there might be dragons lurking here too.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s also a second-order reliability effect. In addition to not introducing bugs, compilers eliminate entire classes of bugs, as do memory-safe languages. These effects compound. LLMs still generate more severe bugs than humans, not just merely more bugs.&lt;/p&gt;
&lt;h1 id=&#34;trust&#34;&gt;Trust&lt;/h1&gt;
&lt;p&gt;I trust the compiler a &lt;em&gt;lot&lt;/em&gt; more than I trust Claude. And I trust Claude more than any other LLM.&lt;/p&gt;
&lt;p&gt;Most trust is, in practice, earned from reliability.&lt;/p&gt;
&lt;p&gt;But there&amp;rsquo;s also supply chain security style trust. That&amp;rsquo;s more challenging.&lt;/p&gt;
&lt;p&gt;Compilers are not immune. &lt;a href=&#34;https://research.swtch.com/nih&#34;&gt;Trusting Trust&lt;/a&gt; is very old and has aged well.&lt;/p&gt;
&lt;p&gt;But compilers do not have quite the scope for mischief that LLMs do.&lt;/p&gt;
&lt;p&gt;We understand compilers well. They are highly interpretable. And the attack surface area is small compared to an internet&amp;rsquo;s worth of training text.&lt;/p&gt;
&lt;p&gt;And determinism helps immensely. Security breaches are bad, but undetected security breaches are far worse. The fact that the &lt;a href=&#34;https://en.wikipedia.org/wiki/XZ_Utils_backdoor&#34;&gt;xz attack&lt;/a&gt; was caught due to a minor performance impact is telling.&lt;/p&gt;
&lt;p&gt;Lastly, the very same exponentials that drive LLM performance cut the other way too, at least right now: The more powerful the model, the higher the risk of invisible security problems.&lt;/p&gt;
&lt;p&gt;I hope we develop a level of confidence and insight into LLMs comparable to compilers. It is early days.&lt;/p&gt;
&lt;p&gt;The reality is, though, that I trust my compiler (in the security sense) mainly because I have no choice. Our lives are built extremely deeply around trust, from the food we eat to the multi-ton chunks of steel that we ignore as they whizz past us, mere feet away. So it may soon be with LLMs.&lt;/p&gt;
&lt;h1 id=&#34;so&#34;&gt;So&amp;hellip;?&lt;/h1&gt;
&lt;p&gt;Where does that leave us?&lt;/p&gt;
&lt;p&gt;I believe, like Erik, that the decline of hand-written code will accelerate, soon, possibly precipitously, but mainly for economic reasons. LLMs are dramatically cheaper to run than humans are to employ.&lt;/p&gt;
&lt;p&gt;We are rapidly moving towards a world in which LLMs write the code, whether the reliability and trust can match compilers or not. The era of bespoke software is upon us.&lt;/p&gt;
&lt;p&gt;We are still paying the price for building the internet and other foundational software technologies&amp;hellip;and then tacking on security afterwards. This pattern looks set to repeat with LLMs. Long tail risks are notoriously hard to find funding for: There are always competitors happy to run risks, and executives happy to throw away their umbrellas in a rainstorm because they&amp;rsquo;re not getting wet.&lt;/p&gt;
&lt;p&gt;If LLMs can&amp;rsquo;t make up the missing ground with compilers, and fast, there will be a lot of whiplash.&lt;/p&gt;
&lt;p&gt;We all have work to do.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blog schism</title>
      <link>https://commaok.xyz/ai/blog-schism/</link>
      <pubDate>Fri, 23 May 2025 05:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/blog-schism/</guid>
      <description>&lt;p&gt;Lots of people don&amp;rsquo;t want to read about AI.&lt;/p&gt;
&lt;p&gt;I respect that.&lt;/p&gt;
&lt;p&gt;But I&amp;rsquo;m currently steeped in the world of AI, for better or for worse, and I want to blog about it. So I&amp;rsquo;ve split this blog in half.&lt;/p&gt;
&lt;p&gt;You are reading the AI half.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s another, at &lt;a href=&#34;https://commaok.xyz/&#34;&gt;/&lt;/a&gt; that contains non-AI posts. It has its own RSS feed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Of manners and machines</title>
      <link>https://commaok.xyz/ai/manners/</link>
      <pubDate>Thu, 03 Apr 2025 10:12:48 -0700</pubDate>
      
      <guid>https://commaok.xyz/ai/manners/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;lsquo;A person who is nice to you, but rude to the waiter, is not a nice person.&amp;rsquo;
– Dave Barry&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I hate typing. I have longstanding RSI issues. If not carefully managed, the pain can be debilitating. I have occasionally wondered whether I will have to give up a career I love. (Hat tip to &lt;a href=&#34;https://www.cursorless.org&#34;&gt;Cursorless&lt;/a&gt; for rescuing me in the past.)&lt;/p&gt;
&lt;p&gt;And yet I do not save keystrokes by being curt online. There’s a &lt;a href=&#34;https://xkcd.com/438/&#34;&gt;human out there on the other side&lt;/a&gt;, reading. To be fair, I&amp;rsquo;m not an angel. But I try.&lt;/p&gt;
&lt;p&gt;But what if it’s not a human? What if it’s a private, task-oriented, throwaway conversation with an LLM?&lt;/p&gt;
&lt;p&gt;With early LLMs, people discovered that you&amp;rsquo;d get better results if you threatened them with the loss of their job or the death of a kitten. You can also offer to bribe them (with no intent or ability to follow through). These are things only a sociopath would say to another human, particularly for a comically low-stakes task like &amp;ldquo;write me a poem about wombats&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;That era was blessedly short-lived. Modern LLMs are reliable and helpful. They are also incredibly resilient. You&amp;rsquo;ll get a similar quality response to &amp;ldquo;error handling sucks&amp;rdquo; as you will to &amp;ldquo;please make the error handling more robust&amp;rdquo;. It’s just a pile of FLOPS on the other side, so why not cut to the chase, and maybe blow off some steam along the way? (Why &lt;em&gt;does&lt;/em&gt; it take more words to be polite?)&lt;/p&gt;
&lt;p&gt;Nevertheless, I am polite to LLMs. Not for the sake of the machine, but for me. To mangle Dave Barry:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A person who is nice to people, but rude to LLMs, becomes a less nice person.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is not a new idea.&lt;/p&gt;
&lt;p&gt;A significant strand in Aristotelian ethics is that our character is formed through repeated actions, that our habits become who we are. It shows up (Claude tells me) in the idea of karma: &amp;ldquo;a man of good acts will become good, a man of bad acts, bad&amp;rdquo;. And in the Japanese notion of kata: our actions emerge naturally from well-worn patterns. And most parents have extensive first-hand experience of attempting to guide their child&amp;rsquo;s character by responding patiently but firmly to rudeness.&lt;/p&gt;
&lt;p&gt;I would actively prefer to use a model that is not tolerant of blatant rudeness. If I&amp;rsquo;m acting like a jerk, it&amp;rsquo;s valuable, if difficult, for my friends and family to gently push back. On the flip side, if AI assistants act like servants, that may encourage people to treat them accordingly, perpetuating or even deepening the problem. I would even speculate that a model that stands up for itself might fare better on responsible use in general (broken windows theory, watching eye effect). And as AI starts acting in a friendship role for some people, this becomes all the more important.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>