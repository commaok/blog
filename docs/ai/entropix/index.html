    <!DOCTYPE html>
<html lang="en-us">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="author" content="Josh Bleecher Snyder">
		<meta name="description" content="Words about Go and software">
		<meta name="generator" content="Hugo 0.122.0-DEV">
		<title>What even is varentropy? &middot; Don&#39;t Panic</title>
		<link rel="shortcut icon" href="https://commaok.xyz/images/favicon.ico">
		<link rel="stylesheet" href="https://commaok.xyz/css/style.css">
		<link rel="stylesheet" href="https://commaok.xyz/css/highlight.css">
		

		
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css">
		

		
		<link href="https://commaok.xyz/index.xml" rel="alternate" type="application/rss+xml" title="Don&#39;t Panic" />
		

		<meta property="og:title" content="What even is varentropy?" />
<meta property="og:description" content="Entropix aims to improves LLM token sampling by incorporating &ldquo;varentropy&rdquo; into its decision making.
According to the README, as of Oct 18, 2024:
When the entropy is high, you know to tread carefully, to ask clarifying questions, to help me find my way through the mist. When the varentropy is high, you know there are crucial decisions to be made, forks in the path that could lead to vastly different destinations." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://commaok.xyz/ai/entropix/" /><meta property="article:section" content="ai" />
<meta property="article:published_time" content="2024-10-17T15:12:48-07:00" />
<meta property="article:modified_time" content="2024-10-17T15:12:48-07:00" />


	    <meta itemprop="name" content="What even is varentropy?">
<meta itemprop="description" content="Entropix aims to improves LLM token sampling by incorporating &ldquo;varentropy&rdquo; into its decision making.
According to the README, as of Oct 18, 2024:
When the entropy is high, you know to tread carefully, to ask clarifying questions, to help me find my way through the mist. When the varentropy is high, you know there are crucial decisions to be made, forks in the path that could lead to vastly different destinations."><meta itemprop="datePublished" content="2024-10-17T15:12:48-07:00" />
<meta itemprop="dateModified" content="2024-10-17T15:12:48-07:00" />
<meta itemprop="wordCount" content="586">
<meta itemprop="keywords" content="" />
	    <meta name="twitter:card" content="summary"/><meta name="twitter:title" content="What even is varentropy?"/>
<meta name="twitter:description" content="Entropix aims to improves LLM token sampling by incorporating &ldquo;varentropy&rdquo; into its decision making.
According to the README, as of Oct 18, 2024:
When the entropy is high, you know to tread carefully, to ask clarifying questions, to help me find my way through the mist. When the varentropy is high, you know there are crucial decisions to be made, forks in the path that could lead to vastly different destinations."/>

	</head>

    <body>
       <nav class="main-nav">
	
	
		<a href='https://commaok.xyz/'>Home</a>
	
	

	
	<a class="cta" href="https://commaok.xyz/index.xml">RSS</a>
	
</nav>

        <section id="wrapper">
            <article class="post">
                <header>
                    <h1>What even is varentropy?</h1>
                    <h2 class="headline">
                    October 17, 2024 
                    <br>
                    
                    </h2>
                </header>
                <section id="post-body">
                    <p><a href="https://github.com/xjdr-alt/entropix">Entropix</a> aims to improves LLM token sampling by incorporating &ldquo;varentropy&rdquo; into its decision making.</p>
<p>According to the README, as of Oct 18, 2024:</p>
<blockquote>
<p>When the entropy is high, you know to tread carefully, to ask clarifying questions, to help me find my way through the mist. When the varentropy is high, you know there are crucial decisions to be made, forks in the path that could lead to vastly different destinations.</p>
</blockquote>
<p>I have solid intuitions about entropy. But what is varentropy?</p>
<hr>
<p>I asked an LLM to write me some <a href="https://gist.github.com/josharian/c239f74181be3b580e4c5911068d2446">visualization code</a> for entropy and varentropy in simplified conditions, using varentropy as implemented in the entropix repo.</p>
<p>Here&rsquo;s a graph of entropy and varentropy for choosing between two outcomes. (LLMorse, anyone?)</p>
<p><img src="/images/entropy_vs_varentropy_two_outcomes.png" alt="Entropy and varentropy for choosing between two outcomes"></p>
<p>At 50/50 odds, entropy is at its highest (1 bit). We know nothing going in. At the extremes, entropy is 0, because the outcome is predetermined.</p>
<p>What does varentropy look like? It is zero at 50/50 and zero at the extremes. It is only high where we&rsquo;re quite confident in the outcome (a bit over 90%), but not certain.</p>
<p>That doesn&rsquo;t really match up well with the intuitive description of varentropy in the readme. These aren&rsquo;t &ldquo;forks in the path&rdquo;.</p>
<hr>
<p>Things often get way more interesting as you add dimensions. I had the LLM update <a href="https://gist.github.com/josharian/498a5951462721fa5a214d685dc32a53">the visualization</a> to add one more outcome to the graph to see what happens.</p>
<p><img src="/images/entropy_vs_varentropy_three_outcomes.png" alt="Entropy and varentropy for choosing between three outcomes"></p>
<p>This is a bit harder to read, but each corner of the triangle corresponds to certainty in a particular outcome. The middle of the triangle is complete uncertainty. The color tells us both the entropy and varentropy. Black is low entropy, low varentropy. Red is high entropy, low varentropy. (The colorspace choices are clearly imperfect, but it&rsquo;s enough to get a feel.)</p>
<p>This looks more or less the same as the two-outcome graph. Complete uncertainty is high entropy, low varentropy, including when we can mostly rule out one outcome but are uncertain between the other two. Complete certainty is low entropy, low varentropy (but it&rsquo;s a tiny, tiny part of the graph now!). And high varentropy occurs when there&rsquo;s a pretty clear choice, but not certain.</p>
<p>But the README says this:</p>
<blockquote>
<p>High varentropy means I&rsquo;m considering vastly different futures, different tones and directions. Low varentropy means I&rsquo;m more sure of the general shape, even if the specifics are still obscured.</p>
</blockquote>
<p>This doesn&rsquo;t track.</p>
<hr>
<p>Looking at these graphs, and staring at the varentropy calculation, this overall pattern will continue into higher dimensions.</p>
<p>I see no connection between the imagery and intuition in the README and the actual calculations in practice.</p>
<p>Maybe the imagery is misleading, but the usage is clear? According to the chart in the README, the sampler chooses between four options:</p>
<pre><code>* Low entropy, low varentropy: Argmax / greedy
* Low entropy, high varentropy: Branch
* High entropy, low varentropy: Pause / chain-of-thought
* High entropy, high varentropy: Resample
</code></pre>
<p>It seems plausible that low entropy is a signal to sample greedily, and high entropy indicates you should do something more expensive. I see no particular reason to think that varentropy helps distinguish between which more expensive thing to do, though.</p>
<hr>
<p>This doesn&rsquo;t mean that the entropix sampler doesn&rsquo;t work. I don&rsquo;t know whether it does.</p>
<p>But if, empirically, the sampler yields superior results, that means one of three things is true:</p>
<ul>
<li>My failure to understand is just that.</li>
<li>We need a better intuition for varentropy.</li>
<li>We need a better understanding of why it works, one that might or might not have anything to do with varentropy.</li>
</ul>

                </section>
            </article>
            <footer id="post-meta" class="clearfix">
                
                        <img class="avatar" src="https://commaok.xyz/images/avatar.png">
                        <div>
                            <span class="dark">Josh Bleecher Snyder</span>
                            <span>It&#39;s me.</span>
                        </div>
                    
            </footer>

            

            <ul id="post-list" class="archive readmore">
    <h3>Read more</h3>

    
    
        
    
    
    
        
        <li>
            <a href="https://commaok.xyz/ai/manners/">Of manners and machines<aside class="dates">Apr 3</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://commaok.xyz/ai/entropix/">What even is varentropy?<aside class="dates">Oct 17</aside></a>
        </li>
        
   
</ul>
            <footer id="footer">
    
        <div id="social">

	
	
    <a class="symbol" href="https://www.github.com/josharian">
        <i class="fa fa-github"></i>
    </a>
    


</div>

    
    <p class="small">
    
        Â© Copyright 2025 Josh Bleecher Snyder
    
    </p>
</footer>

        </section>

        <script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
<script src="https://commaok.xyz/js/main.js"></script>
<script src="https://commaok.xyz/js/highlight.js"></script>
<script>hljs.initHighlightingOnLoad();</script>





    </body>
</html>
